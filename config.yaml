# BitsAndBytesConfig
load_in_4bit: true
# bnb_4bit_compute_dtype": "bf16"

# LoraConfig
r: 8
target_modules:
    [
        "q_proj",
        "o_proj",
        "k_proj",
        "v_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ]
task_type: "CAUSAL_LM"

# TrainingArguments

output_dir: "output_dir" # REQUIRED
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
warmup_steps: 2
max_steps: 100
learning_rate: 2e-4
fp16: true
logging_steps: 1
optim: "paged_adamw_8bit"
